% This file was created with JabRef 2.10.
% Encoding: UTF8


@InProceedings{2014_alshammari,
  Title                    = {Hadoop based enhanced cloud architecture for bioinformatic
algorithms},
  Author                   = {Alshammari, H. and Bajwa, H. and Jeongkyu Lee},
  Booktitle                = {Systems, Applications and Technology Conference (LISAT), 2014
IEEE Long Island},
  Year                     = {2014},
  Month                    = {May},
  Pages                    = {1-5},

  Abstract                 = {Explosion of biological data due to large-scale genomic
research and advances in high throughput data generation tools result in
massive distributed datasets. Analysis of such large non-relational,
heterogeneous, and distributed datasets is emerging challenge in data
driven biomedical industries. Highly complex biological data require
unconventional computational approaches and knowledge-based solutions.
Distributed datasets need to be reduced to smaller datasets that can be
efficiently queried. Since genomic and biological data is generated in
large volume and is stored in geographically diverse locations,
distributed computing on multiple clusters, our objective here is to
assess the feasibility of using Cloud based platform to analyze genomic
big data. In this paper we present an enhanced Hadoop architecture to
reduce computation by utilizing “Common Features” before performing
redundant computation. The enhanced Hadoop architecture allows the jobs
to share these common features among them. The common features describe
the contents of data in blocks and can be used to determine DataNodes
that store the required data.},
  Doi                      = {10.1109/LISAT.2014.6845204},
  Keywords                 = {Big Data;bioinformatics;cloud computing;genomics;DataNodes
determination;Hadoop architecture;bioinformatic algorithms;cloud
architecture;common feature utilization;genomic big data
analysis;Bioinformatics;Biological cells;Computer
architecture;DNA;Distributed databases;File
systems;Genomics;BigData;Hadoop;Hadoop Architecture;MapReduce},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Article{2012_anderson,
  Title                    = {Transfer Function Model for Pollutant Breakthrough in Geomedia},
  Author                   = {S.H. Anderson and Horng-Jer Shieh and R.L. Peyton},
  Journal                  = {Procedia Computer Science },
  Year                     = {2012},
  Note                     = {Complex Adaptive Systems 2012 },
  Pages                    = {236 - 241},
  Volume                   = {12},

  Abstract                 = {Pollutant transport in soil and geologic materials has been evaluated to prevent contamination of groundwater reservoirs. The objective of this project was to use two transfer function models for predicting X-ray computed tomography-measured chemical breakthrough in columns. Breakthrough experiments were conducted using undisturbed cores taken from a field site. Application of the convection-dispersion model is more limited to homogeneous columns while the lognormal and convection-dispersion probability density function (pdf) transfer function models worked well for both heterogeneous as well as homogeneous columns. The parameters for travel time pdf were accurately estimated by the solute breakthrough time distributions when the resolution of the scan was 0.5 x 0.5 mm. However, group-pixel averages of 2.0 x 2.0 mm or 5.0 x 5.0 mm worked best to reduce fluctuations of \{CT\} scan numbers. In this study, the application of the transfer function model was limited to conservative solute transport in undisturbed columns. Future studies can evaluate chemical adsorption and degradation processes. },
  Doi                      = {http://dx.doi.org/10.1016/j.procs.2012.09.062},
  ISSN                     = {1877-0509},
  Keywords                 = {lognormal transfer function},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877050912006539}
}

@InProceedings{2014_anuradha,
  Title                    = {Suggested techniques for clustering and mining of data streams},
  Author                   = {Anuradha, G. and Roy, B.},
  Booktitle                = {Circuits, Systems, Communication and Information Technology
Applications (CSCITA), 2014 International Conference on},
  Year                     = {2014},
  Month                    = {April},
  Pages                    = {265-270},

  Abstract                 = {The buzz word in research is Big Data. Big Data gets
characterized by 5 V's: Volume, Velocity, Variety, Veracity and Value of
data. Volume in order of penta bytes, velocity which refers to click
stream data in various domains, variety comprising of heterogeneous
data, veracity indicating the cleanliness of data and value emphasizing
on the return on investment for companies who invest in Big Data
technologies. This Big Data is better modeled not as persistent tables
but in the form of transient data streams which need different
clustering and mining techniques to be effectively processed and
managed. In this paper some suggestions on online learning through
clustering and mining of stream data are presented.},
  Doi                      = {10.1109/CSCITA.2014.6839270},
  Keywords                 = {Big Data;data mining;pattern clustering;Big Data
technology;buzz word;data stream clustering technique;data stream mining
technique;data value;data variety;data velocity;data veracity;data
volume;heterogeneous data;online learning;return on investment;transient
data streams;Algorithm design and analysis;Big data;Classification
algorithms;Clustering algorithms;Data mining;Heuristic
algorithms;Prediction algorithms;Big Data;Clustering;Data Streams;Mining},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@InProceedings{2012_aoki,
  Title                    = {Study on Knowledge Management Platform about the Field of
Agricultural Infomatization},
  Author                   = {Aoki, E. and Kudo, K. and Fukuda, A. and Nakanishi, T. and
Tagashira, S. and Okayasu, T. and Tsuruda, N. and Yamasaki, S. and
Imura, Y.},
  Booktitle                = {Complex, Intelligent and Software Intensive Systems (CISIS),
2012 Sixth International Conference on},
  Year                     = {2012},
  Month                    = {July},
  Pages                    = {705-710},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {In the region of Japan, broadband network and informatization
have spread. However the field of agriculture has not come willingly.
The infrastructure that has not been enough in mountainous areas, but
various initiatives of computerization are being started using
production and distribution as the subjects. Since this began in
reality, there are still insufficient matters that have been resolved in
regards to the technical validations and cost effectiveness. Therefore
necessitates pouring in a tremendous amount of expertise, possibility is
ICT and its related technologies of sensors. In our research and
developments, we create demo hardware for the experiment using in the
farm collected data. On the other hand, making study group with farmers
for accumulation of data and experience. That visualized information
configure to the platform of knowledge management.},
  Doi                      = {10.1109/CISIS.2012.149},
  Keywords                 = {agriculture;broadband networks;knowledge management;research
and development;sensors;ICT;Japan;agricultural infomatization;broadband
network;cost effectiveness;knowledge management platform;research and
development;sensor;technical validation;Agriculture;Data
visualization;Monitoring;Production;Research and
development;Sensors;Temperature distribution;Agricultural
infomatization;SNS;Sensor network},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@InProceedings{2015_apostol,
  Title                    = {Towards a Hybrid Local-Cloud Framework for Smart Farms},
  Author                   = {Apostol, E. and Leordeanu, C. and Mocanu, M. and Cristea, V.},
  Booktitle                = {Control Systems and Computer Science (CSCS), 2015 20th
International Conference on},
  Year                     = {2015},
  Month                    = {May},
  Pages                    = {820-824},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {Smart or intelligent farming is a relatively new concept, but
it's becoming an important factor for the agricultural sectors, as a way
to raise productivity through technology. This article presents a global
management and decision system for smart farming. Our system is
addressed to farmers and other actors involved in farming operations,
such as rural service companies and banks. It can manage up to several
groups of smart or technology-based farms. The system is functioning on
two levels, local and Cloud. First of all, it must be deployed on the
farm's local wireless network. And secondly, it uses the Cloud's
resources and flexibility to offer even more complex services.},
  Doi                      = {10.1109/CSCS.2015.122},
  Keywords                 = {agriculture;cloud computing;decision support
systems;productivity;resource allocation;agricultural sector;cloud
resource utilization;decision system;farm productivity;global
management;hybrid local-cloud framework;intelligent farming;smart
farming;Agriculture;Cloud
computing;Clouds;Meteorology;Monitoring;Sensors;Agricultural
services;Cloud systems;Data processing;Smart farms},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Electronic{2012_arnold,
  Title                    = {The big deal about big data.},
  Author                   = {Arnold, E.},
  Month                    = {October},
  Organization             = {Information today},
  Year                     = {2012},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.22}
}

@Article{2015_barmpounakis,
  Title                    = {Management and control applications in Agriculture domain via a Future Internet Business-to-Business platform},
  Author                   = {Sokratis Barmpounakis and Alexandros Kaloxylos and Aggelos Groumas and Lampros Katsikas and Vasileios Sarris and Konstantina Dimtsa and Fabiana Fournier and Eleni Antoniou and Nancy Alonistioti and Sjaak Wolfert},
  Journal                  = {Information Processing in Agriculture },
  Year                     = {2015},
  Number                   = {1},
  Pages                    = {51 - 63},
  Volume                   = {2},

  Abstract                 = {Abstract The Agriculture business domain, as a vital part of the overall supply chain, is expected to highly evolve in the upcoming years via the developments, which are taking place on the side of the Future Internet. This paper presents a novel Business-to-Business collaboration platform from the agri-food sector perspective, which aims to facilitate the collaboration of numerous stakeholders belonging to associated business domains, in an effective and flexible manner. The contemporary \{B2B\} collaboration schemes already place the requirements for swift deployment of cloud applications, capable of both integrating diverse legacy systems, as well as developing in a rapid way new services and systems, which will be able to instantly communicate and provide complete, âfarm-to-forkâ solutions for farmers, agri-food and logistics service providers, \{ICT\} companies, end-product producers, etc. To this end, this conceptual paper describes how these requirements are addressed via the \{FIspace\} \{B2B\} platform, focusing on the Greenhouse Management &amp; Control scenarios.},
  Doi                      = {http://dx.doi.org/10.1016/j.inpa.2015.04.002},
  ISSN                     = {2214-3173},
  Keywords                 = {Future Internet},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S2214317315000153}
}

@Conference{2012_begoli,
  Title                    = {Design principles for effective knowledge discovery from big data},
  Author                   = {Begoli, E. and Horey, J.},
  Booktitle                = {Joint Working {IEEE}/{IFIP} Conference on Software Architecture and European Conference on Software Architecture},
  Year                     = {2012},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.22}
}

@Article{2013_bhole,
  Title                    = {3D segmentation of abdominal CT imagery with graphical models, conditional random fields and learning},
  Author                   = {Bhole, Chetan
and Pal, Christopher
and Rim, David
and Wism{\"u}ller, Axel},
  Journal                  = {Machine Vision and Applications},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {301--325},
  Volume                   = {25},

  Abstract                 = {Probabilistic graphical models have had a tremendous impact in machine learning and approaches based on energy function minimization via techniques such as graph cuts are now widely used in image segmentation. However, the free parameters in energy function-based segmentation techniques are often set by hand or using heuristic techniques. In this paper, we explore parameter learning in detail. We show how probabilistic graphical models can be used for segmentation problems to illustrate Markov random fields (MRFs), their discriminative counterparts conditional random fields (CRFs) as well as kernel CRFs. We discuss the relationships between energy function formulations, MRFs, CRFs, hybrids based on graphical models and their relationships to key techniques for inference and learning. We then explore a series of novel 3D graphical models and present a series of detailed experiments comparing and contrasting different approaches for the complete volumetric segmentation of multiple organs within computed tomography imagery of the abdominal region. Further, we show how these modeling techniques can be combined with state of the art image features based on histograms of oriented gradients to increase segmentation performance. We explore a wide variety of modeling choices, discuss the importance and relationships between inference and learning techniques and present experiments using different levels of user interaction. We go on to explore a novel approach to the challenging and important problem of adrenal gland segmentation. We present a 3D CRF formulation and compare with a novel 3D sparse kernel CRF approach we call a relevance vector random field. The method yields state of the art performance and avoids the need to discretize or cluster input features. We believe our work is the first to provide quantitative comparisons between traditional MRFs with edge-modulated interaction potentials and CRFs for multi-organ abdominal segmentation and the first to explore the 3D adrenal gland segmentation problem. Finally, along with this paper we provide the labeled data used for our experiments to the community.},
  Doi                      = {10.1007/s00138-013-0497-x},
  ISSN                     = {1432-1769},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28},
  Url                      = {http://dx.doi.org/10.1007/s00138-013-0497-x}
}

@Article{2007_biolchini,
  Title                    = {Scientific research ontology to support systematic review in software engineering},
  Author                   = {Biolchini, J. C. A. and Mian, P. G. and Natali, A. C. C. and Conte, T. U. and Travassos, G. H.},
  Journal                  = {Advanced {E}ngineering {I}nformatics},
  Year                     = {2007},
  Volume                   = {21},

  Doi                      = {doi:10.1016/j.aei.2006.11.006},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.26}
}

@Conference{2014_blair,
  Title                    = {High performance data management and analysis for tomography},
  Author                   = {Blair, J. and Canon, R.S. and Deslippe, J. and Essiari, A. and Hexemer, A. and MacDowell, A.A. and Parkinson, D.Y. and Patton, S.J. and Ramakrishnan, L. and Tamura, N. and Tierney, B.L. and Tull, C.E.},
  Year                     = {2014},
  Note                     = {cited By 0},
  Volume                   = {9212},

  Abstract                 = {The Advanced Light Source (ALS) is a third-generation synchrotron X-ray source that operates as a user facility with more than 40 beamlines hosting over 2000 users per year from around the world. Users of the Hard X-ray Micro-Tomography Beamline (8.3.2) often collect more than 1 Terabyte of raw data per day that in turn generates additional Terabytes of processed data. The data rate continues to increase rapidly due to faster detectors and new sample automation capabilities. We will present the development and deployment of a computational pipeline, fed by data from the ALS, and powered by the storage, networking, and computing resources of the local National Energy Research Scientific Computing Center (NERSC) and the Energy Sciences Network (ESNET). After one year of operation, the system contained 70,000 datasets and 350 TB of data from 85 users. All datasets now collected at the Hard X-ray Tomography Beamline are automatically reconstructed using parameters set by users and/or that are automatically detected from the data acquisition control system. Results are presented to users for visualization through a secure web portal. Users can then download their data or launch a (currently limited but) growing number of operations based on the data-such as filtering, segmentation, and simulation. The massive computational resources of NERSC are thus made available on a level that is easily accessible to the full range of micro-tomography users. Â© 2014 SPIE.},
  Affiliation              = {Lawrence Berkeley National Lab., Berkeley, CA, United States},
  Art_number               = {92121G},
  Author_keywords          = {Big data; High performance computing; Light source; MicroCT; Supercomputer; Synchrotron; Tomography; Workflow},
  Comment                  = {Somente abstract.},
  Document_type            = {Conference Paper},
  Doi                      = {10.1117/12.2069862},
  Journal                  = {Proceedings of SPIE - The International Society for Optical Engineering},
  Owner                    = {Gabriel},
  Source                   = {Scopus},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84923052612&partnerID=40&md5=d3ea8c492d159df2b9202e358d96f0b6}
}

@Article{2014_blas,
  Title                    = {Surfing the optimization space of a multiple-GPU parallel implementation of a X-ray tomography reconstruction algorithm},
  Author                   = {Javier Garcia Blas and Monica Abella and Florin Isaila and Jesus Carretero and Manuel Desco},
  Journal                  = {Journal of Systems and Software },
  Year                     = {2014},
  Pages                    = {166 - 175},
  Volume                   = {95},

  Abstract                 = {Abstract The increasing popularity of massively parallel architectures based on accelerators have opened up the possibility of significantly improving the performance of X-ray computed tomography (CT) applications towards achieving real-time imaging. However, achieving this goal is a challenging process, as most \{CT\} applications have not been designed for exploiting the amount of parallelism existing in these architectures. In this paper we present the massively parallel implementation and optimization of Mangoose++, a \{CT\} application for reconstructing 3D volumes from 2D images collected by scanners based on cone-beam geometry. The main contribution of this paper are the following. First, we develop a modular application design that allows to exploit the functional parallelism inside the application and to facilitate the parallelization of individual application phases. Second, we identify a set of optimizations that can be applied individually and in combination for optimally deploying the application on a massively parallel multi-GPU system. Third, we present a study of surfing the optimization space of the modularized application and demonstrate that a significant benefit can be obtained from employing the adequate combination of application optimizations. },
  Doi                      = {http://dx.doi.org/10.1016/j.jss.2014.03.083},
  ISSN                     = {0164-1212},
  Keywords                 = {\{CT\} reconstruction},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121214001034}
}

@Article{2013_blas,
  Title                    = {Parallel implementation of a X-ray tomography Reconstruction algorithm based on MPI and CUDA},
  Author                   = {Blas, J. G. and Abella, M. and Liria, E. and Isaila, F. and Carretero, J. and Desco, M.},
  Journal                  = {EuroMPI},
  Year                     = {2013},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Article{2015_bolon-canedo,
  Title                    = {Recent advances and emerging challenges of feature selection in the context of big data},
  Author                   = {V. Bol\'{o}n-Canedo and N. S\'{a}nchez-Maro\~{n}o and A. Alonso-Betanzos},
  Journal                  = {Knowledge-Based Systems },
  Year                     = {2015},
  Pages                    = {33 - 45},
  Volume                   = {86},

  Abstract                 = {Abstract In an era of growing data complexity and volume and the advent of big data, feature selection has a key role to play in helping reduce high-dimensionality in machine learning problems. We discuss the origins and importance of feature selection and outline recent contributions in a range of applications, from \{DNA\} microarray analysis to face recognition. Recent years have witnessed the creation of vast datasets and it seems clear that these will only continue to grow in size and number. This new big data scenario offers both opportunities and challenges to feature selection researchers, as there is a growing need for scalable yet efficient feature selection methods, given that existing methods are likely to prove inadequate. },
  Doi                      = {http://dx.doi.org/10.1016/j.knosys.2015.05.014},
  ISSN                     = {0950-7051},
  Keywords                 = {Feature selection},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950705115002002}
}

@Article{2012_callebaut,
  Title                    = {Scientific perspectivism: a philosopher of science's response to the challenge of big data biology.},
  Author                   = {Callebaut, W.},
  Journal                  = {Studies in history and philosophy of biological sciences},
  Year                     = {2012},
  Volume                   = {43},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.22}
}

@Book{cgi_2015,
  Title                    = {Pesquisa sobre o uso das tecnologias da informa\c{c}\~{a}o e comunica\c{c}\~{a}o nos dom\'{i}cilios brasileiros: TIC Dom\'{i}cilios 2014.},
  Author                   = {CGI.br},
  Editor                   = {Alexandre F. Barbosa},
  Publisher                = {Comit\^{e} Gestor da Internet no Brasil},
  Year                     = {2015},

  Address                  = {S\~{a}o Paulo},
  Month                    = {Novembro},

  Org-short                = {CGI.br},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.18}
}

@Article{2014_chen,
  Title                    = {Data-intensive applications, challenges, techniques and technologies: a survey on {B}ig {D}ata.},
  Author                   = {Chen, C. L. P. and Zhang, C.},
  Journal                  = {Information Sciences},
  Year                     = {2014},

  Month                    = {August},
  Pages                    = {314-347},
  Volume                   = {275},

  Doi                      = {http://dx.doi.org/10.1016/j.ins.2014.01.015},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.22}
}

@Article{2012_chen,
  Title                    = {Business intelligence and analytics: from big data to big impact.},
  Author                   = {Chen, H. and Chiang, R. H. L. and Storey, V. C.},
  Journal                  = {{MIS} Quartely},
  Year                     = {2012},
  Number                   = {4},
  Volume                   = {36},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.22}
}

@Article{2015_conti,
  Title                    = {From \{MANET\} to people-centric networking: Milestones and open research challenges},
  Author                   = {Marco Conti and Chiara Boldrini and Salil S. Kanhere and Enzo Mingozzi and Elena Pagani and Pedro M. Ruiz and Mohamed Younis},
  Journal                  = {Computer Communications },
  Year                     = {2015},
  Pages                    = { - },

  Abstract                 = {Abstract In this paper, we discuss the state of the art of (mobile) multi-hop ad hoc networking with the aim to present the current status of the research activities and identify the consolidated research areas, with limited research opportunities, and the hot and emerging research areas for which further research is required. We start by briefly discussing the \{MANET\} paradigm, and why the research on \{MANET\} protocols is now a cold research topic. Then we analyze the active research areas. Specifically, after discussing the wireless-network technologies, we analyze four successful ad hoc networking paradigms, mesh networks, opportunistic networks, vehicular networks, and sensor networks that emerged from the \{MANET\} world. We also present an emerging research direction in the multi-hop ad hoc networking field: people centric networking, triggered by the increasing penetration of the smartphones in everyday life, which is generating a people-centric revolution in computing and communications. },
  Doi                      = {http://dx.doi.org/10.1016/j.comcom.2015.09.007},
  ISSN                     = {0140-3664},
  Keywords                 = {MANET},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0140366415003412}
}

@InProceedings{2015_jnaneshwar,
  Title                    = {Devices, systems, and methods for automated monitoring enabling
precision agriculture},
  Author                   = {Das, Jnaneshwar and Cross, Gareth and Qu, Chao and Makineni,
Anurag and Tokekar, Pratap and Mulgaonkar, Yash and Kumar, Vijay},
  Booktitle                = {Automation Science and Engineering (CASE), 2015 IEEE
International Conference on},
  Year                     = {2015},
  Month                    = {Aug},
  Pages                    = {462-469},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {Addressing the challenges of feeding the burgeoning world
population with limited resources requires innovation in sustainable,
efficient farming. The practice of precision agriculture offers many
benefits towards addressing these challenges, such as improved yield and
efficient use of such resources as water, fertilizer and pesticides. We
describe the design and development of a light-weight, multi-spectral 3D
imaging device that can be used for automated monitoring in precision
agriculture. The sensor suite consists of a laser range scanner,
multi-spectral cameras, a thermal imaging camera, and navigational
sensors. We present techniques to extract four key data products — plant
morphology, canopy volume, leaf area index, and fruit counts — using the
sensor suite. We demonstrate its use with two systems: multi-rotor micro
aerial vehicles and on a human-carried, shoulder-mounted harness. We
show results of field experiments conducted in collaboration with
growers and agronomists in vineyards, apple orchards and orange groves.},
  Doi                      = {10.1109/CoASE.2015.7294123},
  Keywords                 = {Agriculture;Cameras;Image reconstruction;Monitoring;Robot
sensing systems;Three-dimensional displays;Vegetation},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@InProceedings{2014_ditter,
  Title                    = {On the Way to Big Data Applications in Industrial Computed
Tomography},
  Author                   = {Ditter, A. and Fey, D. and Schon, T. and Oeckl, S.},
  Booktitle                = {Big Data (BigData Congress), 2014 IEEE International Congress
on},
  Year                     = {2014},
  Month                    = {June},
  Pages                    = {792-793},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {Computed Tomography (CT) has been around, especially in the
medical field, for more than 20 years. Although, the mathematical
foundations for CT were known more than a century ago, technical
limitations delayed its practical application for more than 70 years.
Today, we can build CT systems large enough to scan an entire car, yet,
for the processing of the resulting data we are facing a "Big (sensor)
Data Problem". We currently do not have suitable methods and tools and
cannot handle the large amount of data with conventional
state-of-the-art techniques. As industrial CT became more and more
prevalent over the last few years, especially due its unique features in
the field of non destructive testing, we are proposing and evaluating
the use of new methods, work flows and technologies, such as Cloud
Computing, in order to provide suitable solutions for handling the
steadily growing amount of data and its efficient processing.},
  Doi                      = {10.1109/BigData.Congress.2014.125},
  Keywords                 = {cloud computing;computerised tomography;image
reconstruction;nondestructive testing;3D reconstruction;CT
systems;acquisition time;big data applications;cloud
computing;industrial CT;industrial computed tomography;medical
field;nondestructive testing;scanned object;Big data;Computed
tomography;Linear accelerators;Streaming media;Three-dimensional
displays;Wide area networks;X-ray imaging;compression;industrial
computed tomography;nondestructive testing;sensor data;streaming},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@InProceedings{2015_dlodlo,
  Title                    = {The internet of things in agriculture for sustainable rural
development},
  Author                   = {Dlodlo, N. and Kalezhi, J.},
  Booktitle                = {Emerging Trends in Networks and Computer Communications
(ETNCC), 2015 International Conference on},
  Year                     = {2015},
  Month                    = {May},
  Pages                    = {13-18},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {Rural areas in South Africa and Zambia face a number of
similar issues in the domains of agriculture, connectivity, water,
transport, health and education etc., which calls for potentially
similar solutions to be directed towards solving these issues. The
intention of this research is to investigate the potential contributions
of internet of things technologies (IoT) towards poverty reduction in
these rural areas, in line with the needs identified in these
communities and with emphasis on agriculture. The paper identifies
examples of IoTs to mitigate the agricultural needs of these communities
for the domains of crop farming, weather forecasting, wildlife
management, forestry, livestock farming, market identification and rural
financing.},
  Doi                      = {10.1109/ETNCC.2015.7184801},
  Keywords                 = {Internet of Things;agriculture;sustainable
development;Internet of Things;IoT;South Africa;Zambia;agriculture;crop
farming;forestry;livestock farming;market identification;poverty
reduction;rural financing;sustainable rural development;weather
forecasting;wildlife
management;Agriculture;Animals;Communities;Internet;Rural
areas;Temperature sensors;agriculture;internet of things;rural
development},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Article{2014_dutta,
  Title                    = {Development of an intelligent environmental knowledge system for sustainable agricultural decision support},
  Author                   = {Dutta, R.a and Morshed, A.a and Aryal, J.b and D'Este, C.a and Das, A.c},
  Journal                  = {Environmental Modelling and Software},
  Year                     = {2014},
  Note                     = {cited By 2},
  Pages                    = {264-272},
  Volume                   = {52},

  Abstract                 = {The purpose of this research was to develop a knowledge recommendation architecture based on unsupervised machine learning and unified resource description framework (RDF) for integrated environmental sensory data sources. In developing this architecture, which is very useful for agricultural decision support systems, we considered web based large-scale dynamic data mining, contextual knowledge extraction, and integrated knowledge representation methods. Five different environmental data sources were considered to develop and test the proposed knowledge recommendation framework called Intelligent Environmental Knowledgebase (i-EKbase); including Bureau of Meteorology SILO, Australian Water Availability Project, Australian Soil Resource Information System, Australian National Cosmic Ray Soil Moisture Monitoring Facility, and NASA's Moderate Resolution Imaging Spectroradiometer. Unsupervised clustering techniques based on Principal Component Analysis (PCA), Fuzzy-C-Means (FCM) and Self-organizing map (SOM) were used to create a 2D colour knowledge map representing the dynamics of the i-EKbase to provide "prior knowledge" about the integrated knowledgebase. Prior availability of recommendations from the knowledge base could potentially optimize the accessibility and usability issues related to big data sets and minimize the overall application costs. RDF representation has made i-EKbase flexible enough to publish and integrate on the Linked Open Data cloud. This newly developed system was evaluated as an expert agricultural decision support for sustainable water resource management case study in Australia at Tasmania with promising results. Â© 2013.},
  Affiliation              = {Intelligent Sensing and Systems Laboratory, Commonwealth Scientific and Industrial Research Organisation (CSIRO), Hobart 7001, Australia; School of Geography and Environmental Studies, University of Tasmania, Hobart 7001, Australia; School of Engineering, University of Tasmania, Hobart 7001, Australia},
  Author_keywords          = {Feature; I-EKbase; Knowledge integration; Linked Open Data cloud; Semantic matching},
  Document_type            = {Article},
  Doi                      = {10.1016/j.envsoft.2013.10.004},
  Owner                    = {Gabriel},
  Source                   = {Scopus},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84890435004&partnerID=40&md5=5aedf5f22c5b362015c34fea3931b9a1}
}

@Article{2015_emani,
  Title                    = {Understandable {B}ig {D}ata: {A} {S}urvey},
  Author                   = {Emani, C. K. and Cullot, N. and Nicolle, C.},
  Journal                  = {{C}omputer {S}cience {R}eview},
  Year                     = {2015},
  Pages                    = {70 -- 81},
  Volume                   = {17},

  Doi                      = {http://dx.doi.org/10.1016/j.cosrev.2015.05.002},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.25}
}

@Book{2008_ferreira,
  Title                    = {Miniaur\' {e}lio: o minidicion\'{a}rio da l\'{i}ngua portuguesa},
  Author                   = {Ferreira, A. B. H.},
  Editor                   = {Marina Baird Ferreira},
  Publisher                = {Ed. Positivo},
  Year                     = {2008},

  Address                  = {Curitiba},
  Edition                  = {7},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.19}
}

@Article{2014_gonzalez-huici,
  Title                    = {A Comparative Study of GPR Reconstruction Approaches for Landmine
Detection},
  Author                   = {Gonzalez-Huici, M.A. and Catapano, I. and Soldovieri, F.},
  Journal                  = {Selected Topics in Applied Earth Observations and Remote
Sensing, IEEE Journal of},
  Year                     = {2014},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {4869-4878},
  Volume                   = {7},

  Abstract                 = {This paper compares the performance of three reconstruction
techniques frequently applied to process ground penetrating radar (GPR)
data in the specific context of landmine detection: Stolt migration,
backprojection (BP), and microwave tomographic inversion (MWT). The
detection results provided by these algorithms are contrasted with the
ones obtained from typically adopted GPR data filtering procedures. To
carry out the analysis, we use experimental data collected at a
specifically prepared test field, where different targets were buried at
shallow depths in inhomogeneous soil. The efficiency of the investigated
approaches is quantitatively evaluated in terms of detection accuracies
(ROC curves) obtained applying a single pixel-based and an averaged
energy detection algorithm. Based on this analysis, we found that the
MWT outperforms the other reconstruction algorithms for the smallest
mines, which are the most difficult to detect. On the other hand, MWT
and BP reconstruction techniques achieve comparable performances against
medium mines, and do not improve the outcome in case of big mines with
respect to the filtering approaches. Finally, Stolt migration produces
the worst results for both medium and small mines.},
  Doi                      = {10.1109/JSTARS.2014.2321276},
  ISSN                     = {1939-1404},
  Keywords                 = {filtering theory;ground penetrating radar;image
reconstruction;landmine detection;radar detection;radar imaging;BP;GPR
reconstruction approach;MWT;ROC curve;Stolt
migration;backprojection;data collection;data filtering;ground
penetrating radar;inhomogeneous soil;landmine detection;microwave
tomographic inversion;radar detection;single pixel-based averaged energy
detection algorithm;Antennas;Ground penetrating radar;Image
reconstruction;Landmine detection;Permittivity;Soil measurement;Ground
penetrating radar (GPR);landmine detection;reconstruction algorithms},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Misc{2012_gordon,
  Title                    = {Big data: a big opportunity for librarians.},

  Author                   = {Gordon-murnane, L.},
  HowPublished             = {Online},
  Year                     = {2012},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.22}
}

@Article{2011_hapca,
  Title                    = {Automated statistical method to align 2D chemical maps with 3D X-ray computed micro-tomographic images of soils },
  Author                   = {Simona M. Hapca and Zi X. Wang and Wilfred Otten and Clare Wilson and Philippe C. Baveye},
  Journal                  = {Geoderma },
  Year                     = {2011},
  Number                   = {3â4},
  Pages                    = {146 - 154},
  Volume                   = {164},

  Abstract                 = {Recent 2-dimensional measurements reveal that soils are chemically very heterogeneous at nanometric and micrometric scales. Direct measurement techniques are still lacking to extend these observations to 3 dimensions. Sequential sectioning of soils, followed by 2-dimensional mapping of chemical elements and geometric interpolation to 3D, appears to be the only available alternative. Unfortunately, sectioning of soil samples suffers from geometric distortions that are difficult to avoid in practise. In this regard, the objective of the research described in this article was to develop a procedure enabling one to locate, in a 3D X-ray microtomographic image of a soil sample, a physical surface that is obtained by sectioning and for which a number of chemical maps are available. This procedure involves three steps: (1) the reconstitution of the physical structure of the soil layer surface, (2) the alignment of the chemical maps with the reconstituted soil surface image, and (3) the 3D alignment of the 2D chemical maps with the internal structure of the soil cube. Visual comparison of the C and Si maps and of the reconstituted \{CT\} images of the layer surfaces suggests a good correspondence between them, which is supported by Pearson correlation coefficients of â 0.57, â 0.58, 0.45, and 0.43 for the different surfaces and elements considered. Relative to the original 3D X-ray \{CT\} image of the soil sample, the planes associated with the C and Si maps, respectively, are nearly superposed, which further confirms the validity of the alignment procedure. },
  Doi                      = {http://dx.doi.org/10.1016/j.geoderma.2011.05.018},
  ISSN                     = {0016-7061},
  Keywords                 = {X-ray CT},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S001670611100156X}
}

@Article{2015_holzworth,
  Title                    = {Agricultural production systems modelling and software: Current status and future prospects },
  Author                   = {Dean P. Holzworth and Val Snow and Sander Janssen and Ioannis N. Athanasiadis and Marcello Donatelli and Gerrit Hoogenboom and Jeffrey W. White and Peter Thorburn},
  Journal                  = {Environmental Modelling \& Software },
  Year                     = {2015},
  Pages                    = {276 - 286},
  Volume                   = {72},

  Abstract                 = {Abstract During the past decade, the application of agricultural production systems modelling has rapidly expanded while there has been less emphasis on model improvement. Cropping systems modelling has become agricultural modelling, incorporating new capabilities enabling analyses in the domains of greenhouse gas emissions, soil carbon changes, ecosystem services, environmental performance, food security, pests and disease losses, livestock and pasture production, and climate change mitigation and adaptation. New science has been added to the models to support this broadening application domain, and new consortia of modellers have been formed that span the multiple disciplines. There has not, however, been a significant and sustained focus on software platforms to increase efficiency in agricultural production systems research in the interaction between the software industry and the agricultural modelling community. This paper describes the changing agricultural modelling landscape since 2002, largely from a software perspective, and makes a case for a focussed effort on the software implementations of the major models. },
  Doi                      = {http://dx.doi.org/10.1016/j.envsoft.2014.12.013},
  ISSN                     = {1364-8152},
  Keywords                 = {Agricultural modelling},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1364815214003703}
}

@Article{2013b_houston,
  Title                    = {Adaptive-window indicator kriging: A thresholding method for computed tomography images of porous media },
  Author                   = {A.N. Houston and W. Otten and P.C. Baveye and S. Hapca},
  Journal                  = {Computers \& Geosciences },
  Year                     = {2013},
  Pages                    = {239 - 248},
  Volume                   = {54},

  Abstract                 = {Image segmentation is a crucial step in understanding the structure of porous materials, subsequent analyses being profoundly dependent upon segmentation accuracy. Computed tomography images of naturally occurring heterogeneous materials such as soils are particularly challenging to segment reliably, due to the prevalence of partial volume effects, noise, and other artefacts induced during the image acquisition process. As a result, boundaries between classes of objects, typically pore versus solid in the case of soil, are difficult to identify. Indicator kriging can address these problems in a robust fashion but the computational cost can be very great under some circumstances; this is particularly true under conditions that occur regularly in images of soil. The kriging window size parameter is decisive in obtaining a good quality result at reasonable cost, but is difficult to estimate for an image exhibiting significant heterogeneity. This work demonstrates that, allowing the kriging window size to adapt locally throughout the image provides a very efficient solution. Moreover, it is shown that this can be achieved using a conceptually simple mechanism that involves negligible extra processing cost. The adaptive-window indicator kriging method described in this study achieves easily an order of magnitude improvement in computational performance over a fixed size window implementation without sacrificing quality. In addition, it is shown that, by improving the locality of estimation, the new method is robust when applied to soil images. },
  Doi                      = {http://dx.doi.org/10.1016/j.cageo.2012.11.016},
  ISSN                     = {0098-3004},
  Keywords                 = {Local segmentation},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0098300412003949}
}

@Article{2013_houston,
  Title                    = {Effect of scanning and image reconstruction settings in X-ray computed microtomography on quality and segmentation of 3D soil images },
  Author                   = {A.N. Houston and S. Schmidt and A.M. Tarquis and W. Otten and P.C. Baveye and S.M. Hapca},
  Journal                  = {Geoderma },
  Year                     = {2013},
  Pages                    = {154 - 165},
  Volume                   = {207â208},

  Abstract                 = {Abstract Over the last decade, X-ray computed tomography (CT) has been used increasingly to characterise the microscale architecture of soils. As a result significant progress has been made in the acquisition and interpretation of X-ray \{CT\} data, as well as in the thresholding of 3D greyscale \{CT\} images in order to produce binary (black and white) ones. Nevertheless, sizeable uncertainties persist, in particular concerning optimal instrumental settings used to generate the greyscale images. In this context, the key aim of this study is to investigate in detail the effect of scanning resolution and reconstruction settings such as noise reduction and 32-bit to 8-bit mapping interval on the 3D X-ray \{CT\} imaging of soil structure and the impact on the performance of thresholding methods. To assess the quality of the X-ray \{CT\} greyscale images, measures of contrast, noise and sharpness are proposed and tested on a series of images of five different soil samples. At the same time, performance of four segmentation algorithms, i.e., three methods recently developed to deal specifically with soil samples and Otsu's method as a benchmark, was evaluated using functional measures of 3D binary images, including Minkowski functionals and surface pore connected fraction. Results of these analyses suggest that the acquisition and reconstruction parameters investigated significantly affect the quality of soil images, and the subsequent thresholding process. In particular, it was found that thresholding by any of the four methods is greatly affected by the quality of image sharpness, which for soil images appears to be mainly controlled by the scanning resolution. As a result, it is concluded that no matter what reconstruction resolution is required in a study, in order to allow an accurate identification of the pore space, the sample should always be scanned at the highest resolution permitted by the scanning instrument and the sample size. Results also suggest that the three segmentation methods recently developed for soil images thresholding are robust to different levels of noise as well as the choice of the 32-bit mapping interval, as long as lower and upper interval limits for mapping are chosen within suitable boundaries. },
  Doi                      = {http://dx.doi.org/10.1016/j.geoderma.2013.05.017},
  ISSN                     = {0016-7061},
  Keywords                 = {Soil imaging},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0016706113001717}
}

@Article{2009_jacobs,
  Title                    = {The pathologies of big data.},
  Author                   = {Jacobs, A.},
  Journal                  = {Communications of the {ACM}.},
  Year                     = {2009},
  Number                   = {8},
  Volume                   = {52},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.22}
}

@Article{2008_kaestner,
  Title                    = {Imaging and image processing in porous media research },
  Author                   = {A. Kaestner and E. Lehmann and M. Stampanoni},
  Journal                  = {Advances in Water Resources },
  Year                     = {2008},
  Note                     = {Quantitative links between porous media structures and flow behavior across scales },
  Number                   = {9},
  Pages                    = {1174 - 1187},
  Volume                   = {31},

  Abstract                 = {Three-dimensional imaging and image processing has become an important part for investigations of fluid distribution and flow in porous media. We describe two methods of computed tomography with different characteristics, namely X-ray- and neutron-based. We give an overview of image processing sequences and their methods. We investigated image enhancement with a focus on filters using partial differential equations, classification and structure identification that we used to prepare our images for quantitative evaluations. These methods are demonstrated on a partially saturated sand sample. Finally, we show an application with soil aggregates where investigations using synchrotron X-rays and thermal neutrons have led to new insights and refined fluid distribution and flow models. },
  Doi                      = {http://dx.doi.org/10.1016/j.advwatres.2008.01.022},
  ISSN                     = {0309-1708},
  Keywords                 = {Image enhancement},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0309170808000304}
}

@Article{2012_kaloxylos,
  Title                    = {Farm management systems and the Future Internet era},
  Author                   = {Alexandros Kaloxylos and Robert Eigenmann and Frederick Teye and Zoi Politopoulou and Sjaak Wolfert and Claudia Shrank and Markus Dillinger and Ioanna Lampropoulou and Eleni Antoniou and Liisa Pesonen and Huether Nicole and Floerchinger Thomas and Nancy Alonistioti and George Kormentzas},
  Journal                  = {Computers and Electronics in Agriculture },
  Year                     = {2012},
  Pages                    = {130 - 144},
  Volume                   = {89},

  Abstract                 = {Smart/precision farming systems are expected to play an important role in improving farming activities. During the past years, sophisticated farm management systems have emerged to replace outdated complex and monolithic farm systems and software tools. The latest trend is to enable these management systems to operate over the Internet. However, the Internet, in its current operation form, faces a number of shortcomings especially in handling vast numbers of networked devices (i.e., Internet of Things) or allowing a simplified integration of systems and services developed by different players. Currently, a number of research initiatives aim at addressing these shortcomings. Such an example is the âFuture Internetâ program launched by the European Commission. In the context of our work, we have specified a farm management system that takes advantage of the new characteristics that âFuture Internetâ offers. These come in terms of generic software modules that can be used to build farming related specialized modules. We present the functional architecture of this farm management system and provide an operational example. We also analyze the technological enablers that will make this architecture a reality.},
  Doi                      = {http://dx.doi.org/10.1016/j.compag.2012.09.002},
  ISSN                     = {0168-1699},
  Keywords                 = {Farm management system},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0168169912002219}
}

@InProceedings{2015_karmas,
  Title                    = {Scalable Geospatial Web Services through Efficient, Online and
Near Real-Time Processing of Earth Observation Data},
  Author                   = {Karmas, A. and Tzotsos, A. and Karantzalos, K.},
  Booktitle                = {Big Data Computing Service and Applications (BigDataService),
2015 IEEE First International Conference on},
  Year                     = {2015},
  Month                    = {March},
  Pages                    = {194-201},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {The current generation of space-borne sensors are generating
nearly continuous streams of massive earth observation datasets. These
huge multi-modal streams increase at astonishing rates, reaching
currently several petabytes in many satellite archives. Shortly,
high-resolution multispectral images will be available almost once a
week and in some regions twice per week. However, it is estimated that
most of datasets in existing archives have never been accessed and
processed outside high-end supercomputing and data-center environments.
To this end, a scalable geospatial platform has been designed, developed
and evaluated for the online and real-time harvesting of valuable
information from big earth observation data. The core of our platform
consists of the Rasdaman Array Database Management System for big raster
data storage, and the Open Geospatial Consortium Web Coverage Processing
Service for data querying. The WebGIS client is based on the OpenLayers
and GeoExt JavaScript libraries, while advanced remote sensing and
computer vision libraries like GDAL, Oreo Toolbox and OpenCV have been
integrated as well. Currently, the system is fully covering Greece with
LANDSAT 8 multispectral data, from the beginning of the satellite's
mission. Datasets are stored and pre-processed automatically in our
hardware. The developed system has been validated for the efficient and
automated processing of high resolution satellite data and in particular
for key geospatial, environmental, agriculture and water engineering
applications like precision agriculture, water quality monitoring and
land cover mapping.},
  Doi                      = {10.1109/BigDataService.2015.49},
  Keywords                 = {Java;Web services;database management systems;geographic
information systems;image resolution;query processing;real-time
systems;GeoExt JavaScript libraries;LANDSAT 8 multispectral
data;OpenLayers;Rasdaman array database management system;WebGIS;data
querying;data-center environments;earth observation data;high-end
supercomputing;high-resolution multispectral images;multimodal
streams;near real-time processing;online processing;open geospatial
consortium Web coverage processing service;satellite archives;scalable
geospatial Web services;space-borne
sensors;Agriculture;Earth;Estimation;Geospatial analysis;Remote
sensing;Satellites;Vegetation mapping;Agriculture;Big
Data;Environment;Land Cover;Remote Sensing;Water Quality},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Conference{2014_khan,
  Title                    = {Seven {V}'s of {B}ig {D}ata},
  Author                   = {Khan, M. A. and Uddin, M. F. and Gupta, N.},
  Booktitle                = {American Society for Engineering Education, Proceedings of 2014 Zone 1 Conference of the},
  Year                     = {2014},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.23}
}

@Conference{2014_kim,
  Title                    = {A semantics-oriented storage model for big heterogeneous RDF data},
  Author                   = {Kim, H.S. and Ravindra, P. and Anyanwu, K.},
  Year                     = {2014},
  Note                     = {cited By 0},
  Pages                    = {437-440},
  Volume                   = {1272},

  Abstract                 = {Increasing availability of RDF data covering different domains is enabling ad-hoc integration of different kinds of data to suit varying needs. This usually results in large collections of data such as the Billion Triple Challenge datasets or SNOMED CT, that are not just "big" in the sense of volume but also "big" in variety of property and class types. However, techniques used by most RDF data processing systems fail to scale adequately in these scenarios. One major reason is that the storage models adopted by most of these systems, e.g., vertical partitioning, do not align well with the semantic units in the data and queries. While Big Data distributed processing platforms such as the Hadoopbased platforms offer the promise of "unlimited scale-out processing", there are still open questions as to how best to physically partition and distribute RDF data for optimized distributed processing. In this poster, we present the idea of a semantics-oriented RDF storage model that partitions data into logical units that map to subqueries in graph patterns. These logical units can be seen as equivalence classes of star subgraphs in an RDF graph. This logical partitioning strategy enables more aggressive pruning of irrelevant query results by pruning irrelevant partitions. It also enables the possibility of semantic-query optimization for some queries such as eliminating joins under appropriate conditions. These benefits in addition to appropriate techniques for physically partitioning the logical partitions, translate to improved performance as shown by some preliminary results.},
  Affiliation              = {Department of Computer Science, North Carolina State University, Raleigh, NC, United States},
  Author_keywords          = {Hadoop; MapReduce; Partitioning scheme; RDF storage model},
  Comment                  = {Somente abstract.},
  Document_type            = {Conference Paper},
  Journal                  = {CEUR Workshop Proceedings},
  Owner                    = {Gabriel},
  Source                   = {Scopus},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84921889222&partnerID=40&md5=0f15640815100851c1abc986570a5463}
}

@TechReport{2004_kitchenham,
  Title                    = {Procedures for performing systematic reviews.},
  Author                   = {Kitchenham, B.},
  Institution              = {Keele {U}niversity and {NICTA}},
  Year                     = {2004},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.26}
}

@Conference{2013_lajara,
  Title                    = {Investimentos em {TI} e sua influ\^{e}ncia no desempenho organizacional: atrav\'{e}s da governan\c{c}a da informa\c{c}\~ao e capacidades de gest\~{a}o da informa\c{c}\~ao, moderado pelo big data.},
  Author                   = {Lajara, T. T. and Brinkhues, R. and Ma\c{c}ada, A. C. G.},
  Booktitle                = {Administra\c{c}\~{a}o da Informa\c{c}\~{a}o, IV Encontro de},
  Year                     = {2013},

  Address                  = {Bento Gon\c{c}alves, RS},
  Month                    = {maio},
  Publisher                = {EnADI},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.19}
}

@Article{1994_levine,
  Title                    = {Relationships between soil properties and vegetation at the Northern Experimental Forest, Howland, Maine},
  Author                   = {E.R. Levine and R.G. Knox and W.T. Lawrence},
  Journal                  = {Remote Sensing of Environment },
  Year                     = {1994},
  Note                     = {Remote Sensing of Forest Ecosystems },
  Number                   = {2},
  Pages                    = {231 - 241},
  Volume                   = {47},

  Abstract                 = {This research relates the results of a survey and detailed analysis of soils in a northern mixed conifer forest to vegetation characteristics as represented by remotely sensed data. The work was conducted at International Paper's Northern Experimental Forest (NEF) at Howland, Maine as part of NASA's Forest Ecosystem Dynamics (FED) project. An intensive soil survey was performed and relationships between soil properties (i.e., drainage class, depth of active zone, water holding capacity, carbon / nitrogen ratio, pH, and sum of bases), species composition, and normalized difference vegetation index (NDVI) from the Advanced Visible and Infrared Imaging Spectrometer (AVIRIS) were derived. Results showed that there was great variability in soil properties across the landscape due to complex regional glacial activity and recent alluvial events. Significant statistical differences were observed in species composition and \{NDVI\} between soil mapping units and with soil drainage class. However, other specific soil properties could not be used to explain these differences given the number of soil samples characterized, or without taking disturbance and management history into account. Simulation modeling, which would include soil data and stand history information as inputs, would provide an additional means of interpreting the relationship between remotely sensed imagery, inferred ecosystem properties, and complex, landscape-level patterns of soil characteristics.},
  Doi                      = {http://dx.doi.org/10.1016/0034-4257(94)90158-9},
  ISSN                     = {0034-4257},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0034425794901589}
}

@Article{2015_li,
  Title                    = {Special issue on precision agriculture},
  Author                   = {Minzan Li and Sun-Ok Chung},
  Journal                  = {Computers and Electronics in Agriculture },
  Year                     = {2015},
  Note                     = {Precision Agriculture },
  Pages                    = {1 - },
  Volume                   = {112},

  Doi                      = {http://dx.doi.org/10.1016/j.compag.2015.03.014},
  ISSN                     = {0168-1699},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0168169915000897}
}

@Article{2015_liebeskind,
  Title                    = {Big and bigger data in endovascular stroke therapy},
  Author                   = {Liebeskind, D.S.},
  Journal                  = {Expert Review of Neurotherapeutics},
  Year                     = {2015},
  Note                     = {cited By 0},
  Number                   = {4},
  Pages                    = {335-337},
  Volume                   = {15},

  Abstract                 = {More than 30 years after initial reports demonstrated the feasibility of intra-arterial or endovascular therapies for the treatment of acute ischemic stroke, big data have finally established requisite evidence for the safety and efficacy of thrombectomy. Cautious enthusiasm for this breakthrough is tempered, as we await the bigger data of these trials to understand the constellation of variables that ensured success. Noninvasive imaging, including multimodal computed tomography and MRI as used in recent endovascular trials, has dramatically advanced since that time, providing snapshots or profiles of the collaterome in a given patient. Data-driven analyses will provide the most potent argument to distinguish comprehensive stroke centers from interventional-ready sites. These trials may provide insight on the future role of telestroke, for intravenous thrombolysis and remote imaging review of multimodal computed tomography or MRI to streamline patient transfer for endovascular therapy. Rather than concluding that recent trials have answered the most important question regarding endovascular therapy, even more data are needed to effectively translate such success and extend such potential benefit to the greatest number of stroke patients encountered on a daily basis. Â© 2015 Informa UK, Ltd..},
  Affiliation              = {Neurovascular Imaging Research Core, UCLA Department of Neurology, 635 Charles E Young Drive South, Los Angeles, CA, United States},
  Author_keywords          = {collaterals; collaterome; endovascular; stroke; thrombectomy},
  Document_type            = {Review},
  Doi                      = {10.1586/14737175.2015.1018893},
  Owner                    = {Gabriel},
  Source                   = {Scopus},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84925848738&partnerID=40&md5=ee79d2ce1c57afd7a05458e736f6aba1}
}

@Article{2014_lu,
  Title                    = {Model-based Iterative Reconstruction: A Promising Algorithm for Today's Computed Tomography Imaging},
  Author                   = {Liu, Lu},
  Journal                  = {Journal of Medical Imaging and Radiation Sciences},
  Year                     = {2014},
  Number                   = {2},
  Volume                   = {45},

  Doi                      = {http://dx.doi.org/10.1016/j.jmir.2014.02.002},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28}
}

@Conference{2013_ludena,
  Title                    = {A {B}ig {D}ata approach for a new {ICT} agriculture application development.},
  Author                   = {Ludena, D. A. R. and Ahrary, A.},
  Booktitle                = {Cyber-enabled distributed computing and knowledge discovery, International Conference on},
  Year                     = {2013},

  Doi                      = {http://dx.doi.org/10.1109/CyberC.2013.30},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.22}
}

@Article{2012_mark,
  Title                    = {The importance of 'Big Data': a definition.},
  Author                   = {Mark, A. B. and Laney, D.},
  Journal                  = {Gartner},
  Year                     = {2012},

  Month                    = {June},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.22}
}

@Book{mayer_2013,
  Title                    = {Big Data: como extrair volume, variedade, velocidade e valor da avalanche de informa\c{c}\~ao cotidiana.},
  Author                   = {Viktor Mayer-Schonberger and Kenneth Cukier},
  Editor                   = {Elsevier},
  Publisher                = {Elsevier},
  Year                     = {2013},

  Address                  = {Rio de Janeiro},
  Edition                  = {1},
  Note                     = {tradu\c{c}\~{a}o Paulo Polzonoff Junior.},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.18}
}

@TechReport{2007_montebelo,
  Title                    = {{SRAT}({S}ystematic {R}eview {A}utomatic {T}ool) -- {U}ma ferramenta computacional de apoio \'{a} revis\~{a}o sistem\'{a}tica.},
  Author                   = {Montebelo, R. and Orlando, A. and Porto, D. and Zaniro, D. and Fabbri, S.},
  Institution              = {{UFSC}ar -- {D}epartamento de {C}omputa\c{c}\~{a}o.},
  Year                     = {2007},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.26}
}

@Article{2015_moraes,
  Title                    = {Editorial: {C}omputational {I}ntelligence {A}pplications for {D}ata {S}cience.},
  Author                   = {Moraes, R. M. and Mart\'{i}nez, L.},
  Journal                  = {Knowledge-{B}ased {S}ystems},
  Year                     = {2015},
  Volume                   = {87},

  Doi                      = {http://dx.doi.org/10.1016/j.knosys.2015.07.038},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.25}
}

@InProceedings{2014_murakami,
  Title                    = {iFarm: Development of Web-Based System of Cultivation and Cost
Management for Agriculture},
  Author                   = {Murakami, Y.},
  Booktitle                = {Complex, Intelligent and Software Intensive Systems (CISIS),
2014 Eighth International Conference on},
  Year                     = {2014},
  Month                    = {July},
  Pages                    = {624-627},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {Precision agriculture is aimed at optimizing farming
management and it requires records of agricultural work. Farmers
conventionally write records on paper but it is difficult and tedious to
check past agricultural-work data and control the cost of agricultural
products. A system of cultivation and cost management, iFarm, is
proposed, which was developed to support efficient farming management.
The system consists of smartphone applications, Web browsers and a cloud
server. Farmers on farmland can easily refer to work plans, enter field
data into the cloud system, and share them with head office in real time
by using smartphones. Farmers at head office can analyze data in the
cloud system with a Web browser and estimate farming costs and form work
plans based on their analyses.},
  Doi                      = {10.1109/CISIS.2014.89},
  Keywords                 = {agricultural products;agriculture;cloud computing;cost
accounting;online front-ends;smart phones;Web browsers;Web-based
system;agricultural products;cloud server;cost
management;cultivation;farming management;iFarm;precision
agriculture;smartphone
applications;Agriculture;Browsers;Chemicals;Databases;History;Servers;Synchronization;Cost
management;Cultivation management;Precision agriculture;Smartphone;Web
database},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Article{2013_narula,
  Title                    = {Are We Up to Speed?: From Big Data to Rich Insights in \{CV\} Imaging for a Hyperconnected World},
  Author                   = {Jagat Narula},
  Journal                  = {JACC: Cardiovascular Imaging },
  Year                     = {2013},
  Number                   = {11},
  Pages                    = {1222 - 1224},
  Volume                   = {6},

  Doi                      = {http://dx.doi.org/10.1016/j.jcmg.2013.09.007},
  ISSN                     = {1936-878X},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1936878X13006566}
}

@TechReport{2015_nist,
  Title                    = {NIST Big Data Interoperability Framework: volume 1, definitions. Final version 1.},
  Author                   = {NIST},
  Institution              = {National Institute of Standards and Technology},
  Year                     = {2015},

  Address                  = {Gaithersburg, MD},
  Month                    = {September},

  Doi                      = {http://dx.doi.org/10.6028/NIST.SP.1500-1},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.19}
}

@InProceedings{2015_nitu,
  Title                    = {Reducing Risks in Agriculture},
  Author                   = {Nitu, C. and Dumitrascu, A. and Krapivin, V.F. and Mkrtchyan,
F.A.},
  Booktitle                = {Control Systems and Computer Science (CSCS), 2015 20th
International Conference on},
  Year                     = {2015},
  Month                    = {May},
  Pages                    = {941-945},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {Agricultural risks are discussed because of the disasters
related to the water regime disturbance. Different tools are considered
in framework of the remote-sensing and in-situ monitoring. Global
Information-Modeling System (GIMS) is proposed as effective information
technology for monitoring data analysis and processing. The GIMS basis
is given. Two versions of optical systems are considered for the water
quality diagnostics. Algorithm is proposed to the optical spectral
images recognition. Examples of simulation experiments with the
GIMS-based tools are given.},
  Doi                      = {10.1109/CSCS.2015.87},
  Keywords                 = {agriculture;data analysis;disasters;image
recognition;information systems;information technology;remote
sensing;risk analysis;GIMS;agricultural risks;disasters;global
information-modeling system;in-situ monitoring;information
technology;monitoring data analysis;optical spectral images
recognition;remote-sensing;risk reduction;water regime
disturbance;Microwave measurement;Microwave radiometry;Radar;Remote
sensing;Soil moisture;Vegetation mapping;agriculture;microwave
monitoring;recognition;spectral image;spectroellipsometer;water quality},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Article{2015_ojha,
  Title                    = {Wireless sensor networks for agriculture: The state-of-the-art in practice and future challenges},
  Author                   = {Ojha, T.a b and Misra, S.a and Raghuwanshi, N.S.b },
  Journal                  = {Computers and Electronics in Agriculture},
  Year                     = {2015},
  Note                     = {cited By 0},
  Pages                    = {66-84},
  Volume                   = {118},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {The advent of Wireless Sensor Networks (WSNs) spurred a new direction of research in agricultural and farming domain. In recent times, WSNs are widely applied in various agricultural applications. In this paper, we review the potential WSN applications, and the specific issues and challenges associated with deploying WSNs for improved farming. To focus on the specific requirements, the devices, sensors and communication techniques associated with WSNs in agricultural applications are analyzed comprehensively. We present various case studies to thoroughly explore the existing solutions proposed in the literature in various categories according to their design and implementation related parameters. In this regard, the WSN deployments for various farming applications in the Indian as well as global scenario are surveyed. We highlight the prospects and problems of these solutions, while identifying the factors for improvement and future directions of work using the new age technologies. © 2015 Elsevier B.V.},
  Affiliation              = {School of Information Technology, Indian Institute of Technology Kharagpur, India; Department of Agricultural and Food Engineering, Indian Institute of Technology Kharagpur, India},
  Art_number               = {3348},
  Author_keywords          = {Agriculture; Agriculture in india; Automation; Sensors and actuators; Wireless sensor networks},
  Document_type            = {Review},
  Doi                      = {10.1016/j.compag.2015.08.011},
  Owner                    = {Gabriel},
  Source                   = {Scopus},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84941123409&partnerID=40&md5=f97d1c4e751e6b80e39d41953a5e0730}
}

@Article{2015_perez,
  Title                    = {A semi-supervised system for weed mapping in sunflower crops using unmanned aerial vehicles and a crop row detection method},
  Author                   = {M. P\'{e}rez-Ortiz and J.M. Pe\~{n}a and P.A. Guti\'{e}rrez and J. Torres-S\'{a}nchez and C. Herv\'{a}s-Mart\'{i}nez and F. L\'{o}pez-Granados},
  Journal                  = {Applied Soft Computing },
  Year                     = {2015},
  Pages                    = {533 - 544},
  Volume                   = {37},

  Abstract                 = {Abstract This paper presents a system for weed mapping, using imagery provided by unmanned aerial vehicles (UAVs). Weed control in precision agriculture is based on the design of site-specific control treatments according to weed coverage. A key component is precise and timely weed maps, and one of the crucial steps is weed monitoring, by ground sampling or remote detection. Traditional remote platforms, such as piloted planes and satellites, are not suitable for early weed mapping, given their low spatial and temporal resolutions. Nonetheless, the ultra-high spatial resolution provided by \{UAVs\} can be an efficient alternative. The proposed method for weed mapping partitions the image and complements the spectral information with other sources of information. Apart from the well-known vegetation indexes, which are commonly used in precision agriculture, a method for crop row detection is proposed. Given that crops are always organised in rows, this kind of information simplifies the separation between weeds and crops. Finally, the system incorporates classification techniques for the characterisation of pixels as crop, soil and weed. Different machine learning paradigms are compared to identify the best performing strategies, including unsupervised, semi-supervised and supervised techniques. The experiments study the effect of the flight altitude and the sensor used. Our results show that an excellent performance is obtained using very few labelled data complemented with unlabelled data (semi-supervised approach), which motivates the use of weed maps to design site-specific weed control strategies just when farmers implement the early post-emergence weed control. },
  Doi                      = {http://dx.doi.org/10.1016/j.asoc.2015.08.027},
  ISSN                     = {1568-4946},
  Keywords                 = {Remote sensing},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1568494615005281}
}

@Article{2015_pang,
  Title                    = {Special issue on dimensionality reduction for visual big data},
  Author                   = {Yanwei Pang and Ling Shao},
  Journal                  = {Neurocomputing },
  Year                     = {2015},
  Pages                    = { - },

  Doi                      = {http://dx.doi.org/10.1016/j.neucom.2015.06.088},
  ISSN                     = {0925-2312},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925231215012060}
}

@Conference{2015_patil,
  Title                    = {3-D Image Analysis Using MapReduce},
  Author                   = {Patil, J. and Mane, S.},
  Booktitle                = {Pervasive Computing (ICPC), International Conference on},
  Year                     = {2015},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28}
}

@Book{2014_queiros,
  Title                    = {An\'{a}lise das possibilidades e tend\^{e}ncias do uso das tecnologias da informa\c{c}\~ao e comunica\c{c}\~{a}o em Agricultura de Precis\~{a}o},
  Author                   = {Leonardo Ribeiro Queir\'{o}s and Ariovaldo Luchiari Junior and Jo\~{a}o Camargo Neto and S\'{i}lvia Maria Fonseca Silveira Massruh\'{a} and Ricardo Yassushi Inamasu and Eduardo Antonio Speranza and Silvio Roberto Medeiros Evangelista},
  Editor                   = {Editora Cubo},
  Publisher                = {Embrapa},
  Year                     = {2014},

  Booktitle                = {Agricultura de Precis\~{a}o: resultados de um novo olhar},
  Owner                    = {Gabriel},
  Pages                    = {97 - 108},
  Timestamp                = {2016.02.19}
}

@InProceedings{2015_rahman,
  Title                    = {A Hybrid Network Architecture for Data Centers},
  Author                   = {Rahman, M.N. and Esmailpour, A.},
  Booktitle                = {Big Data Computing Service and Applications (BigDataService),
2015 IEEE First International Conference on},
  Year                     = {2015},
  Month                    = {March},
  Pages                    = {7-13},

  Abstract                 = {In recent years, the concept of Big Data (BD) analytics has
brought rapid changes to the way enterprises conduct business. Everyday
more organizations are getting into the cloud of BD users. Due to an
influx of data analytic projects, data centers and cloud-based service
providers have experienced massive growth. A growth engulfed with
impetuous increase of data and the rapid rise of Internet traffic has
resulted in data centers facing new challenges. At the same time,
service providers must meet the demands of cloud computing and emerging
web applications. To fulfill those requirements, number of servers in a
rack are increasing quickly with servers connected to high bandwidth
switches. Present data center networks based on electrical packet
switching have many disadvantages. Electrical links consume much power
and have bottleneck issues. They are often not suitable to provide low
latency at high throughput. A promising solution involves optical
circuit switch links in the data center. Research in optical fibers has
gained attention for providing high throughput, low latency, and
incredibly low energy consumption. In this paper, we present a hybrid
electrical and optical networking topology for data centers used in
cloud computing and Big Data applications. Furthermore, we explain the
reasons how the proposed architecture helps in improvements related to
the performance of the data centers.},
  Doi                      = {10.1109/BigDataService.2015.43},
  Keywords                 = {Big Data;circuit switching;cloud computing;computer
centres;data analysis;optical fibres;packet switching;telecommunication
traffic;BD analytics;Big Data analytics;Internet traffic;Web
application;cloud computing;cloud-based service provider;data analytic
project;data center;electrical packet switching;high bandwidth
switch;hybrid electrical topology;hybrid network architecture;optical
circuit switch link;optical fiber;optical networking topology;Big
data;Computer architecture;Databases;Optical fiber communication;Optical
switches;Servers;Throughput;Big Data;Cloud;Data Center
Network;Electrical Packet Switch;Optical Circuit Switch},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@InProceedings{2014_rios,
  Title                    = {Big Data Infrastructure for analyzing data generated by Wireless
Sensor Networks},
  Author                   = {Rios, L.G. and Diguez, J.A.I.},
  Booktitle                = {Big Data (BigData Congress), 2014 IEEE International Congress
on},
  Year                     = {2014},
  Month                    = {June},
  Pages                    = {816-823},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {The rapid pace of technological advances in recentyears has
enabled a significant evolution and deployment ofWireless Sensor
Networks (WSN). These networks are a keyplayer in the so-called Internet
of Things, generating exponentiallyincreasing amounts of data.
Nonetheless, there are veryfew documented works that tackle the
challenges related with thecollection, manipulation and exploitation of
the data generated bythese networks. This paper presents a proposal for
integrating BigData tools (in rest and in motion) for the gathering,
storage andanalysis of data generated by a WSN that monitors air
pollutionlevels in a city. We provide a proof of concept that
combinesHadoop and Storm for data processing, storage and analysis,and
Arduino-based kits for constructing our sensor prototypes.},
  Doi                      = {10.1109/BigData.Congress.2014.142},
  Keywords                 = {Big Data;Internet of Things;air pollution;data
analysis;microcontrollers;public domain software;sensor
placement;wireless sensor networks;Arduino based kit;Big Data
Infrastructure;Big Data tool;Hadoop;Internet of Things;Storm;WSN
deployment;air pollution monitoring;data analysis;data collection;data
exploitation;data gathering;data manipulation;data processing;data
storage;sensor prototype construction;wireless sensor network
deployment;Big data;Method of moments;Monitoring;Real-time
systems;Storms;Wireless sensor networks;Arduino; Big Data; Hadoop;
Stream Processing;},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Conference{2015_sabarina,
  Title                    = {Lowering data dimensionality in big data for the benefit of precision agriculture},
  Author                   = {Sabarina, K. and Priya, N.},
  Year                     = {2015},
  Note                     = {cited By 0},
  Number                   = {C},
  Pages                    = {548-554},
  Volume                   = {48},

  Abstract                 = {Predictive analytics can be used to make smarter decisions in farming by collecting real-time data on weather, soil and air quality, crop maturity and even equipment and labor costs and availability. This is known as precision agriculture. Big data is expected to play an important role in precision agriculture for managing real-time data analysis with massive streaming data. The data analysis efficiency and throughput would be a challenge with the massive increase in size of big data. The unstructured streaming data received from different agricultural sources would contain multiple dimensions and not the entire content is needed for performing analysis. The core data which is small but that alone enough to represent the entire content should be extracted. This paper explains how to systematically reduce the size of big data by applying a tensor based feature reduction model. The data decomposition and core value extraction is done with the help of IHOSVD algorithm. This way it reduces the overall file size by eliminating unwanted data dimensions. The time involved in data analysis and CPU usage will be significantly reduced when dimensionality reduced data is used in place of raw (unprocessed) data. Â© 2015 The Authors. Published by Elsevier B.V.},
  Affiliation              = {Jerusalem College of Engineering, Chennai, India},
  Author_keywords          = {Big data; Dimensionality reduction; HOSVD; Precision agriculture},
  Document_type            = {Conference Paper},
  Doi                      = {10.1016/j.procs.2015.04.134},
  Journal                  = {Procedia Computer Science},
  Owner                    = {Gabriel},
  Source                   = {Scopus},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84938911728&partnerID=40&md5=919d27de83e6043e140be3a495a73dec}
}

@InProceedings{2013_sagiroglu,
  Title                    = {Big data: A review},
  Author                   = {Sagiroglu, S. and Sinanc, D.},
  Booktitle                = {Collaboration Technologies and Systems (CTS), 2013 International Conference on},
  Year                     = {2013},
  Month                    = {May},
  Pages                    = {42-47},

  __markedentry            = {[Gabriel:6]},
  Abstract                 = {Big data is a term for massive data sets having large, more
varied and complex structure with the difficulties of storing, analyzing
and visualizing for further processes or results. The process of
research into massive amounts of data to reveal hidden patterns and
secret correlations named as big data analytics. These useful
informations for companies or organizations with the help of gaining
richer and deeper insights and getting an advantage over the
competition. For this reason, big data implementations need to be
analyzed and executed as accurately as possible. This paper presents an
overview of big data's content, scope, samples, methods, advantages and
challenges and discusses privacy concern on it.},
  Doi                      = {10.1109/CTS.2013.6567202},
  Keywords                 = {data analysis;data privacy;big data advantage;big data
challenge;big data content;big data method;big data privacy;big data
sample;big data scope;data analysis;data storage;data visualization;Data
handling;Data models;Data storage systems;Information
management;Organizations;Security;big
data;value;variety;velocity;verification;volume},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.18}
}

@InProceedings{sagiroglu_2013,
  Title                    = {Big data: A review},
  Author                   = {Sagiroglu, S. and Sinanc, D.},
  Booktitle                = {Collaboration Technologies and Systems (CTS), 2013 International Conference on},
  Year                     = {2013},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.18}
}

@Article{2010_schluter,
  Title                    = {Segmentation of X-ray microtomography images of soil using gradient masks },
  Author                   = {Steffen Schl\"{u}uter and Ulrich Weller and Hans-J\"{o}rg Vogel},
  Journal                  = {Computers \& Geosciences },
  Year                     = {2010},
  Number                   = {10},
  Pages                    = {1246 - 1251},
  Volume                   = {36},

  Abstract                 = {For many analyses, grey scale images from X-ray tomography and other sources need to be segmented into objects and background which often is a difficult task and afflicted by an arbitrary and subjective choice of threshold values. This is especially true if the volume fraction of objects is small and the histogram becomes unimodal. Bi-level segmentation based on region growing is a promising approach to cope with the fuzzy transition zone between object and background due to the partial volume effect, but until now there is no method to properly determine the required thresholds in case of unimodality. We propose an automatic and robust technique for threshold selection based on edge detection. The method uses gradient masks which are defined as regions of interest for the determination of threshold values. Its robustness is analysed by a systematic performance test and finally demonstrated for the segmentation of pores in different soils using images from X-ray tomography. },
  Doi                      = {http://dx.doi.org/10.1016/j.cageo.2010.02.007},
  ISSN                     = {0098-3004},
  Keywords                 = {Segmentation},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0098300410001457}
}

@Article{2012_shekhar,
  Title                    = {Spatial Big-Data Challenges Intersecting Mobility and Cloud Computing},
  Author                   = {Shekhar, S. and Gunturi, V. and Evans, M. R. and Yang, K.},
  Journal                  = {MobiDE},
  Year                     = {2012},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Conference{2012_singh,
  Title                    = {Big data analytics.},
  Author                   = {Singh, S. and Singh, N.},
  Booktitle                = {Communication, information \& computing technology, International conference on},
  Year                     = {2012},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.22}
}

@Article{2014_slavakis,
  Title                    = {Modeling and {O}ptimization for {B}ig {D}ata {A}nalytics.},
  Author                   = {Slavakis, K. and Giannakis, G. B. and Mateos, G.},
  Journal                  = {IEEE Signal Processing Magazine},
  Year                     = {2014},
  Pages                    = {18 -- 31},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.25}
}

@Article{2014_sonka,
  Title                    = {Big data and the ag sector: More than lots of numbers},
  Author                   = {Sonka, S.},
  Journal                  = {International Food and Agribusiness Management Review},
  Year                     = {2014},
  Note                     = {cited By 1},
  Number                   = {1},
  Pages                    = {1-20},
  Volume                   = {17},

  Abstract                 = {It seems that one can't go through a work day without seeing some mention of Big Data, its application and its potential to have unprecedented impact. The potential for Big Data application in the agricultural sector is examined. The role of analytics and the variety and velocity characteristics of Big Data as they can apply to the sector are stressed. Integration of data and analysis across business and government entities will be needed for successful implementation. The eventual impact of Big Data within the agricultural sector likely will require both organizational and technological innovation. Â© 2014 International Food and Agribusiness Management Association (IFAMA).},
  Affiliation              = {ADM Institute for the Prevention of Postharvest Loss, Professor Emeritus of Agricultural Strategy, University of Illinois, 807 S. Wright Street, Suite 320, Champaign, IL, 61820, United States},
  Author_keywords          = {Agricultural supply chain integration; Big data; Information and communication technology},
  Comment                  = {Não contém arquivo completo, apenas abstract. Excluído, portanto. 18.01.2016},
  Document_type            = {Article},
  Owner                    = {Gabriel},
  Source                   = {Scopus},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84895087802&partnerID=40&md5=31a33e074c89ebc9e9dd765c3fa55fa8}
}

@Article{2007_taina,
  Title                    = {Application of X-ray Computed Tomography to soil science: a literature review},
  Author                   = {Taina, I. A. and Heck, J. and Elliot, T. R.},
  Journal                  = {Canadia Journal Soil Science},
  Year                     = {2007},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28}
}

@Article{2014_tsai,
  Title                    = {Data Mining for Internet of Things: A Survey},
  Author                   = {Chun-Wei Tsai and Chin-Feng Lai and Ming-Chao Chiang and Yang,
L.T.},
  Journal                  = {Communications Surveys Tutorials, IEEE},
  Year                     = {2014},

  Month                    = {First},
  Number                   = {1},
  Pages                    = {77-97},
  Volume                   = {16},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {It sounds like mission impossible to connect everything on the
Earth together via Internet, but Internet of Things (IoT) will
dramatically change our life in the foreseeable future, by making many
"impossibles" possible. To many, the massive data generated or captured
by IoT are considered having highly useful and valuable information.
Data mining will no doubt play a critical role in making this kind of
system smart enough to provide more convenient services and
environments. This paper begins with a discussion of the IoT. Then, a
brief review of the features of "data from IoT" and "data mining for
IoT' is given. Finally, changes, potentials, open issues, and future
trends of this field are addressed.},
  Doi                      = {10.1109/SURV.2013.103013.00206},
  ISSN                     = {1553-877X},
  Keywords                 = {Internet of Things;data mining;Internet of things;data
mining;Clustering algorithms;Data handling;Data mining;Data storage
systems;Information management;Internet;Wireless sensor
networks;Internet of Things;data mining},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@InProceedings{2014_ushizima,
  Title                    = {Structure recognition from high resolution images of ceramic
composites},
  Author                   = {Ushizima, D. and Perciano, T. and Krishnan, H. and Loring, B.
and Bale, H. and Parkinson, D. and Sethian, J.},
  Booktitle                = {Big Data (Big Data), 2014 IEEE International Conference on},
  Year                     = {2014},
  Month                    = {Oct},
  Pages                    = {683-691},

  Abstract                 = {Fibers provide exceptional strength-to-weight ratio
capabilities when woven into ceramic composites, transforming them into
materials with exceptional resistance to high temperature, and high
strength combined with improved fracture toughness. Microcracks are
inevitable when the material is under strain, which can be imaged using
synchrotron X-ray computed micro-tomography (μ-CT) for assessment of
material mechanical toughness variation. An important part of this
analysis is to recognize fibrillar features. This paper presents
algorithms for detecting and quantifying composite cracks and fiber
breaks from high-resolution image stacks. First, we propose recognition
algorithms to identify the different structures of the composite,
including matrix cracks and fibers breaks. Second, we introduce our
package F3D for fast filtering of large 3D imagery, implemented in
OpenCL to take advantage of graphic cards. Results show that our
algorithms automatically identify micro-damage and that the GPU-based
implementation introduced here takes minutes, being 17x faster than
similar tools on a typical image file.},
  Doi                      = {10.1109/BigData.2014.7004292},
  Keywords                 = {X-ray microscopy;ceramics;composite materials;computerised
tomography;fracture toughness;graphics processing units;image
resolution;mechanical engineering
computing;microcracks;μ-CT;GPU;OpenCL;ceramic composites;composite
cracks;computed micro-tomography;fiber breaks;graphic cards;material
mechanical toughness variation;microcracks;microdamage;strain;structure
recognition;synchrotron X-ray;Algorithm design and
analysis;Ceramics;Image processing;Prototypes;Random access
memory;Three-dimensional displays;Fiber Detection;GPU;ImageJ/Fiji
plug-in;Material Inspection;OpenCL},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Article{2011_wang,
  Title                    = {Comparison of image segmentation methods in simulated 2D and 3D microtomographic images of soil aggregates },
  Author                   = {W. Wang and A.N. Kravchenko and A.J.M. Smucker and M.L. Rivers},
  Journal                  = {Geoderma },
  Year                     = {2011},
  Number                   = {3â4},
  Pages                    = {231 - 241},
  Volume                   = {162},

  Abstract                 = {Advances in X-ray microtomography (Î¼CT) are opening new opportunities for examining soil pore structures. However, usefulness of Î¼CT data for pore structure characterization depends on how accurately the grayscale images are segmented into pore and solid components. Multiple segmentation algorithms have been developed; however, one of the difficulties in comparing the accuracy of segmentation algorithms is the lack of ground-truth information in the soil samples subjected to Î¼CT. This means that only the criteria that do not depend on the availability of the ground-truth data can be used in assessing accuracy of the segmentation methods, yet the reliability of such criteria in soil images is unclear. In this study, we simulated 2D and 3D soil images to resolve the problem of the lack of ground-truth information. The objectives of the study were (i) to explore optimal parameter selection for indicator kriging (IK) segmentation; (ii) to compare the accuracy of several commonly used segmentation methods, namely, entropy based method, iterative method, Otsu's method, and \{IK\} method; and (iii) to evaluate performance of the region non-uniformity measure (NU), the criterion that does not depend on presence of the ground-truth image, in segmentation method selection for soil images. We found that though there was no single segmentation method that preserved pore characteristics in all the cases, \{IK\} method yielded segmented images most similar to the ground-truth in most of the cases when the histogram of image grayscale values had clearly distinguishable peaks. For the image with poorly distinguishable histogram peaks, \{IK\} did not perform well, while Otsu's method produced acceptable segmentation results. The results indicated that selecting the segmentation method based on \{NU\} did not always produce optimal representation of pore characteristics. However, overall, the \{NU\} was found to be an acceptable criterion for segmentation method selection in Î¼CT soil images. },
  Doi                      = {http://dx.doi.org/10.1016/j.geoderma.2011.01.006},
  ISSN                     = {0016-7061},
  Keywords                 = {Simulated image},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0016706111000152}
}

@Conference{2014_wu,
  Title                    = {A parallel processing model for big medical image data},
  Author                   = {Wu, M. and Zhou, Y. and Du, Z. and Wu, X.},
  Year                     = {2014},
  Note                     = {cited By 0},
  Pages                    = {266-269},

  Abstract                 = {Parallel computing has gained a great influence on scientific researches and in our daily life, especially when dealing with big data. One of the preconditions of high performance on computing is the support of efficient algorithms, which should be divisible and computing simultaneously. But not all algorithms are applicable for parallel computing, sometimes it can only make use of one single processor. In order to take full advantages of cluster or Multi-core CPUs in that case, A pipeline computation model is proposed which applies on cluster to make procedures more efficient and make full use of computer resources. Especially, our model has a very good performance on medical image process. With the model, almost all the positions of the organs in CT-images of a person could be found out simultaneously and accurately in one time, which can efficiently speed up the diagnosis of doctors, rather than the serial algorithm which can only find the position of one organ in one time before. The result of our experiment shows that the performance of the former serial algorithm has been improved by 40 percent by using our method. Â© 2014 IEEE.},
  Affiliation              = {School of Computer Engineering and Science, Shanghai University, Shanghai, China},
  Art_number               = {7065048},
  Author_keywords          = {medical image; Parallel computing; pipeline computation model},
  Comment                  = {Somente abstract.},
  Document_type            = {Conference Paper},
  Doi                      = {10.1109/ITAIC.2014.7065048},
  Journal                  = {2014 IEEE 7th Joint International Information Technology and Artificial Intelligence Conference, ITAIC 2014},
  Owner                    = {Gabriel},
  Source                   = {Scopus},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84934276685&partnerID=40&md5=c8712578e82a3d09aa4d4f909d343416}
}

@InProceedings{xexeo_2013,
  Title                    = {Big Data: computa\c{c}\~{a}o para uma sociedade conectada e digitalizada},
  Author                   = {Geraldo Xex\'{e}o},
  Booktitle                = {Revista Ci\^{e}ncia Hoje},
  Year                     = {2013},
  Month                    = {Agosto},
  Pages                    = {18 - 23},
  Volume                   = {306},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.18}
}

@Article{2014_yang,
  Title                    = {3D characterization and analysis of pore structure of packed ore particle beds based on computed tomography images },
  Author                   = {Bao-hua YANG and Ai-xiang WU and Xiu-xiu MIAO and Jin-zhi LIU},
  Journal                  = {Transactions of Nonferrous Metals Society of China },
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {833 - 838},
  Volume                   = {24},

  Abstract                 = {Abstract Methods and procedures of three-dimensional (3D) characterization of the pore structure features in the packed ore particle bed are focused. X-ray computed tomography was applied to deriving the cross-sectional images of specimens with single particle size of 1â2, 2â3, 3â4, 4â5, 5â6, 6â7, 7â8, 8â9, 9â10 mm. Based on the in-house developed 3D image analysis programs using Matlab, the volume porosity, pore size distribution and degree of connectivity were calculated and analyzed in detail. The results indicate that the volume porosity, the mean diameter of pores and the effective pore size (d50) increase with the increasing of particle size. Lognormal distribution or Gauss distribution is mostly suitable to model the pore size distribution. The degree of connectivity investigated on the basis of cluster-labeling algorithm also increases with increasing the particle size approximately. },
  Doi                      = {http://dx.doi.org/10.1016/S1003-6326(14)63131-9},
  ISSN                     = {1003-6326},
  Keywords                 = {packed ore particle bed},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1003632614631319}
}

@Article{2015_yang,
  Title                    = {Accessing medical image file with co-allocation \{HDFS\} in cloud},
  Author                   = {Chao-Tung Yang and Wen-Chung Shih and Lung-Teng Chen and Cheng-Ta Kuo and Fuu-Cheng Jiang and Fang-Yie Leu},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2015},
  Pages                    = {61 - 73},
  Volume                   = {43â44},

  Abstract                 = {Abstract Patient privacy has recently become the most important issue in the World Health Organization (WHO) and the United States and Europe. However, inter-hospital medical information is currently shared using paper-based operations, and this is an important research issue for the complete and immediate exchange of electronic medical records to avoid duplicate prescriptions or procedures. An electronic medical record (EMR) is a computerized medical record created by a care-giving organization, such as a hospital and doctorâs surgery. Using electronic medical records can improve patientâs privacy and health care efficiency. Although there are many advantages to electronic medical records, the problem of exchanging and sharing medical images remains to be solved. The motivation of this paper is to attempt to resolve the problems of storing and sharing electronic medical records and medical images between different hospitals. Cloud Computing is enabled by the existing parallel and distributed technology, which provides computing, storage and software services to users. Specifically, this study develops a Medical Image File Accessing System (MIFAS) based on \{HDFS\} of Hadoop in cloud. The proposed system can improve medical imaging storage, transmission stability, and reliability while providing an easy-to-operate management interface. This paper focuses on the cloud storage virtualization technology to achieve high-availability services. We have designed and implemented a medical imaging system with a distributed file system. The experimental results show that the high reliability data storage clustering and fault tolerance capabilities can be achieved. },
  Doi                      = {http://dx.doi.org/10.1016/j.future.2014.08.008},
  ISSN                     = {0167-739X},
  Keywords                 = {EMR},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X14001563}
}

@Article{2015_yang,
  Title                    = {Big Data from {CT} Scanning},
  Author                   = {Yang, Q. and Kalra, M. K. and Padole, A. and Li, J. and Hilliard, E. and Lai, R. and Wang, G.},
  Journal                  = {SM Biomed Imaging Data Papers},
  Year                     = {2015},

  Owner                    = {Gabriel},
  Timestamp                = {2016.02.28}
}

@InProceedings{2014_zhang,
  Title                    = {MaPLE: A MapReduce Pipeline for Lattice-based Evaluation and its
application to SNOMED CT},
  Author                   = {Guo-Qiang Zhang and Wei Zhu and Mengmeng Sun and Shiqiang Tao
and Bodenreider, O. and Licong, Cui},
  Booktitle                = {Big Data (Big Data), 2014 IEEE International Conference on},
  Year                     = {2014},
  Month                    = {Oct},
  Pages                    = {754-759},

  Abstract                 = {Non-lattice fragments are often indicative of structural
anomalies in ontological systems and, as such, represent possible areas
of focus for subsequent quality assurance work. However, extracting the
non-lattice fragments in large ontological systems is computationally
expensive if not prohibitive, using a traditional sequential approach.
In this paper we present a general MapReduce pipeline, called MaPLE
(MapReduce Pipeline for Lattice-based Evaluation), for extracting
non-lattice fragments in large partially ordered sets and demonstrate
its applicability in ontology quality assurance. Using MaPLE in a
30-node Hadoop local cloud, we systematically extracted non-lattice
fragments in 8 SNOMED CT versions from 2009 to 2014 (each containing
over 300k concepts), with an average total computing time of less than 3
hours per version. With dramatically reduced time, MaPLE makes it
feasible not only to perform exhaustive structural analysis of large
ontological hierarchies, but also to systematically track structural
changes between versions. Our change analysis showed that the average
change rates on the non-lattice pairs are up to 38.6 times higher than
the change rates of the background structure (concept nodes). This
demonstrates that fragments around non-lattice pairs exhibit
significantly higher rates of change in the process of ontological
evolution.},
  Doi                      = {10.1109/BigData.2014.7004301},
  Keywords                 = {Big Data;bioinformatics;cloud computing;ontologies (artificial
intelligence);Hadoop local cloud;MaPLE;MapReduce pipeline for
lattice-based evaluation;SNOMED CT;biomedical Big Data;nonlattice
fragment extraction;ontological systems;Algorithm design and
analysis;Context;Lattices;Ontologies;Pipelines;Quality assurance;Upper
bound},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

@Conference{2013_zhang,
  Title                    = {5Ws model for {B}ig{D}ata analysis and visualization},
  Author                   = {Zhang, J. and Huang, M. L.},
  Booktitle                = {Computational Science and Engineering, IEEE 16th International Conference on},
  Year                     = {2013},
  Pages                    = {1021 -- 1028},

  Doi                      = {http://dx.doi.org/10.1109/CSE.2013.149},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.22}
}

@InProceedings{2011_zhang,
  Title                    = {Research of real-time image acquisition system based on ARM 7 for
agricultural environmental monitoring},
  Author                   = {Zhang, Jie and Li, Aicheng and Jianlong Li and Yang, Qi and
Gang, Chengcheng},
  Booktitle                = {Remote Sensing, Environment and Transportation Engineering
(RSETE), 2011 International Conference on},
  Year                     = {2011},
  Month                    = {June},
  Pages                    = {6216-6220},

  __markedentry            = {[gabriel:1]},
  Abstract                 = {The internet of things (IOT) has brought a broader space to
the intensive development of modern agriculture in the application of
agricultural environmental monitoring. To solve the problems of
real-time video acquisition, image acquisition and motion capture in the
agricultural environment, this paper use the ARM S3C44B0X as the
processor and put forward a new kind of real-time video acquisition
technique based on 32bits embedded processor system. the optimized and
minimized hardware system is designed, including designs the GAL
gathering controller to control the output digital video signal data
form SAA7111A to the frame buffer storage memory Al422, and expanded the
interface of FLASH, SDRAM, CF Card, RS232, JTAG and so on; Secondly,
through the transplanting of U-boot and uClinux system, the improved
moving object examination algorithm using the difference of background
was proposed based on the software platform constructed. Finally, the
debug process of the unit circuit of the whole system is introduced in
proper sequence. As a result, the digital image collecting system based
on ARM S3C44B0X exists as a sole unit, and has lots of advantages such
as small volume, low cost, good expansion, multifunctional and low power
consumption. The system realized the functions of real-time video
acquisition and processing. Combined with the GPS module and other
sensors, the system can be widely used in the aspects of agricultural
environmental monitoring, real-time data acquisition and remote
monitoring and so on, with a more broad application prospects.},
  Doi                      = {10.1109/RSETE.2011.5965777},
  Keywords                 = {Agriculture;Clocks;Debugging;Hardware;Monitoring;Real time
systems;Streaming media;ARM;Al422B;S3C44B0X;SAA7111A;agricultural
environmental monitoring;precision agriculture;video acquisition},
  Owner                    = {Gabriel},
  Timestamp                = {2016.02.27}
}

